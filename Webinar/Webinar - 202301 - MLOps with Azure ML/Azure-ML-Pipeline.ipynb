{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure Machine Learning Pipeline\n",
        "\n",
        " In this notebook, we will be creating a Azure Machine Learning Pipeline for the complete stage of machine learning lifecycle:\n",
        " \n",
        " 1. Data Engineering\n",
        " 2. Model Training\n",
        " 3. Model Management\n",
        " 4. Model Deployment (to same environment)\n",
        " \n",
        "![Data Engineering](./images/00-Pipeline.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "### Production Review\n",
        "\n",
        "We will be working with product review dataset. There are 5 classes from software reviews. \n",
        "\n",
        "In this multi-class classification problem, we will use scikit-learning to train a multinomial naive bayes model and use Azure Machine Learning to create experiment, manage models and deploy to web service for consumption.\n",
        "\n",
        "![Sample data](./images/sample_data.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Data Engineering \n",
        "\n",
        "**Input** : Raw Data \n",
        "\n",
        "**Output** : Registered Data Set (ProductReview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Registering the data asset will enable you to:\n",
        "\n",
        "* reuse and share the data asset in future pipelines\n",
        "* use versions to track the modification to the data asset\n",
        "* use the data asset from Azure ML designer, which is Azure ML's GUI for pipeline authoring\n",
        "\n",
        "<img src=\"../e2e-ds-experience/media/dataset.PNG\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673457604396
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('data_engineering',exist_ok=True)\n",
        "os.makedirs('train', exist_ok=True)\n",
        "os.makedirs('model_selection', exist_ok=True)\n",
        "os.makedirs('model_deploy', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1638856840545
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile data_engineering/data_engineering.py\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "\n",
        "from azureml.core.run import Run\n",
        "from azureml.core import Dataset, Datastore, Workspace\n",
        "from azureml.data.datapath import DataPath\n",
        "\n",
        "# get run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# Download data from source\n",
        "url = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Software_5.json.gz\"\n",
        "response = requests.get(url, stream=True,verify=False)\n",
        "\n",
        "with open(\"Software_5.json.gz\", \"wb\") as handle:\n",
        "    for data in response.iter_content():\n",
        "        handle.write(data)\n",
        "\n",
        "### load the meta data\n",
        "data = []\n",
        "with gzip.open('Software_5.json.gz') as f:\n",
        "    for l in f:\n",
        "        data.append(json.loads(l.strip()))\n",
        "    \n",
        "# total length of list, this number equals total number of products\n",
        "print(len(data))\n",
        "\n",
        "# first row of the list\n",
        "print(data[0])\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "\n",
        "### remove rows with unformatted title (i.e. some 'title' may still contain html style content)\n",
        "df3 = df.fillna('')\n",
        "df3.iloc[2]\n",
        "\n",
        "# register dataset\n",
        "workspace = run.experiment.workspace\n",
        "default_datastore = Datastore.get_default(workspace)\n",
        "\n",
        "ds_name = 'ProductReview'\n",
        "data_path = DataPath(datastore=default_datastore, path_on_datastore='product_review')\n",
        "\n",
        "ds = Dataset.Tabular.register_pandas_dataframe(df3, \n",
        "                                    default_datastore, \n",
        "                                    ds_name, \n",
        "                                    description=None, \n",
        "                                    tags=None, \n",
        "                                    show_progress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "![Dataset](./images/01-DataEngineeringOutput.jpb.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Model Training\n",
        "\n",
        "In this step, we are ready to do some experiments and select the best model for production use.\n",
        "\n",
        "**Input** : Register Data Set (ProductReview)\n",
        "\n",
        "**Output** : Trained Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile train/train_2.py\n",
        "# General libraries.\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from azureml.core.run import Run\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from azureml.core import Workspace, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from joblib import dump\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "run = Run.get_context()\n",
        "\n",
        "# Get workspace from run context\n",
        "workspace = run.experiment.workspace\n",
        "\n",
        "# Load Data\n",
        "dataset = Dataset.get_by_name(workspace, name='ProductReview')\n",
        "data = dataset.to_pandas_dataframe()[['overall', 'reviewText']]\n",
        "\n",
        "# Prepare X & Y\n",
        "Y = data.pop('overall').to_numpy()\n",
        "X = data.pop('reviewText').to_numpy()\n",
        "train_x, test_x, train_y, test_y = train_test_split(X,Y, test_size = 0.1, random_state=1)\n",
        "\n",
        "# Fit vectorizer\n",
        "vec = CountVectorizer()\n",
        "fitted_train_data = vec.fit_transform(train_x)\n",
        "fitted_test_data = vec.transform(test_x)\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "params = {'alpha': [1.0e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}\n",
        "\n",
        "clf = GridSearchCV(model, params, scoring = \"f1_macro\", verbose=0, cv = 5)\n",
        "clf_result = clf.fit(fitted_train_data, train_y)\n",
        "run.log(\"Best alpha \",clf_result.best_estimator_.alpha)\n",
        "pred = clf.predict(fitted_test_data)\n",
        "run.log(\"F1\", metrics.f1_score(test_y, pred, average='weighted'))\n",
        "\n",
        "plot_confusion_matrix(clf, fitted_test_data, test_y)  \n",
        "plt.savefig('confusion_matrix.png')\n",
        "run.log_image(name='Confusion-Matrix', path='./confusion_matrix.png')\n",
        "\n",
        "# Save trained model\n",
        "dump(vec, './vec.pkl')\n",
        "dump(clf, './mnb.pkl')\n",
        "run.upload_file(name='vec.pkl', path_or_stream='./vec.pkl')\n",
        "run.upload_file(name='mnb.pkl', path_or_stream='./mnb.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "![CM](./images/confusion_matrix.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Model Selection\n",
        "\n",
        "In this step, we will use a predefined metrics **F1**. We will list all today's runs and select the highest F1 score model, which will be registered in Model Registry and prepare for deployment.\n",
        "\n",
        "**Input** : Trained Model\n",
        "\n",
        "**Output** : Registered Model in Model Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile model_selection/model_select.py\n",
        "import sklearn\n",
        "from datetime import datetime, date\n",
        "from azureml.core.run import Run\n",
        "# from azureml.core import Experiment\n",
        "# from azureml.core.model import Model\n",
        "\n",
        "# get run context\n",
        "run = Run.get_context()\n",
        "workspace = run.experiment.workspace\n",
        "\n",
        "# Get Experiment and runs for model select\n",
        "# In this step, we will use F1\n",
        "exp = run.experiment # Experiment.list(workspace, experiment_name='MLOps-Workshop')\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "select_run = None\n",
        "F1 = 0\n",
        "for r in exp.get_runs():\n",
        "    run_starttime = datetime.strptime(r.get_details()['startTimeUtc'][:10], '%Y-%m-%d').date()\n",
        "    if run_starttime==today:\n",
        "        for step in r.get_children():\n",
        "            current_step_f1 = step.get_metrics(name='F1')\n",
        "            if 'F1' in current_step_f1.keys() and F1<current_step_f1['F1']:\n",
        "                F1=current_step_f1['F1']\n",
        "                select_run = step\n",
        "    \n",
        "if select_run != None:\n",
        "    # Load Data\n",
        "    mnb_model = select_run.register_model(\"ProductReview-NaiveBayes\",\n",
        "                            model_path=\"./mnb.pkl\",\n",
        "                            )\n",
        "\n",
        "    vector    = select_run.register_model(\"ProductReview-CountVector\", \n",
        "                            model_path=\"./vec.pkl\",\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "![Model Register](./images/model_register.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Model Deploy\n",
        "\n",
        "\n",
        "**Input** : Registered Model in Model Registry\n",
        "\n",
        "**Output** : Container Instance / Endpoint (Web Service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile model_deploy/score.py\n",
        "import json, os, joblib\n",
        "from azureml.core.model import Model\n",
        "\n",
        "def init(): \n",
        "  global vec, clf\n",
        "  print(Model.get_model_path('ProductReview-NaiveBayes'))\n",
        "  vec = joblib.load(Model.get_model_path('ProductReview-CountVector'))\n",
        "  clf = joblib.load(Model.get_model_path('ProductReview-NaiveBayes'))\n",
        "\n",
        "def run(data): \n",
        "  input_data = json.loads(data)['data'] \n",
        "  fitted_data = vec.transform(input_data)\n",
        "  pred = clf.predict(fitted_data)\n",
        "  return json.dumps(pred.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile model_deploy/deploy.py\n",
        "from azureml.core.model import InferenceConfig, Model\n",
        "from azureml.core import Environment\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.webservice import AciWebservice\n",
        "\n",
        "# get run context\n",
        "run = Run.get_context()\n",
        "workspace = run.experiment.workspace\n",
        "\n",
        "# inference config\n",
        "service_name = 'product-review-service'\n",
        "env = Environment.get(workspace=workspace, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\")\n",
        "\n",
        "inference_config = InferenceConfig(entry_script='score.py', \n",
        "                            source_directory='.',\n",
        "                            environment=env)\n",
        "\n",
        "# deployment config\n",
        "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
        "\n",
        "nb_model = Model(workspace, 'ProductReview-NaiveBayes')\n",
        "vectorizor = Model(workspace, 'ProductReview-CountVector')\n",
        "\n",
        "service = Model.deploy(\n",
        "    workspace,\n",
        "    name = service_name,\n",
        "    models=[nb_model, vectorizor],\n",
        "    inference_config= inference_config,\n",
        "    deployment_config= deployment_config,\n",
        "    overwrite=True,\n",
        ")\n",
        "service.wait_for_deployment(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5. Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673457995650
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Environment, Experiment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "import os\n",
        "\n",
        "workspace = Workspace.from_config()\n",
        "# 1. Get ComputeTarget\n",
        "aml_compute_target = \"cpu-cluster\"\n",
        "try:\n",
        "    aml_compute = AmlCompute(workspace, aml_compute_target)\n",
        "    print(\"found existing compute target.\")\n",
        "except ComputeTargetException:\n",
        "    print(\"creating new compute target\")\n",
        "    \n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
        "                                                                min_nodes = 1, \n",
        "                                                                max_nodes = 4)    \n",
        "    aml_compute = ComputeTarget.create(workspace, aml_compute_target, provisioning_config)\n",
        "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)    \n",
        "print(\"Azure Machine Learning Compute attached\")\n",
        "\n",
        "# 2. Get environment\n",
        "env = Environment.get(workspace=workspace, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\")\n",
        "# create a new runconfig object\n",
        "runconfig = RunConfiguration()\n",
        "runconfig.environment = env\n",
        "\n",
        "# 3. Define steps\n",
        "dataprep_step = PythonScriptStep( name=\"prep_data\", \n",
        "                                script_name=\"data_engineering.py\", \n",
        "\t                            source_directory=\"data_engineering\", \n",
        "                                compute_target=aml_compute_target, \n",
        "                                runconfig=runconfig                                \n",
        "\t                            )\n",
        "\n",
        "train_step    = PythonScriptStep( name=\"train\", \n",
        "                                script_name=\"train_2.py\", \n",
        "\t                            source_directory=\"train\", \n",
        "                                compute_target=aml_compute_target, \n",
        "                                runconfig=runconfig\n",
        "\t                            )\n",
        "train_step.run_after(dataprep_step)\n",
        "\n",
        "select_step   = PythonScriptStep( name=\"select_model\", \n",
        "                                script_name=\"model_select.py\", \n",
        "\t                            source_directory=\"model_selection\", \n",
        "                                compute_target=aml_compute_target, \n",
        "                                runconfig=runconfig\n",
        "\t                            )\n",
        "select_step.run_after(train_step)\n",
        "\n",
        "deploy_step   = PythonScriptStep( name=\"deploy_model\", \n",
        "                                script_name=\"deploy.py\", \n",
        "\t                            source_directory=\"model_deploy\", \n",
        "                                compute_target=aml_compute_target, \n",
        "                                runconfig=runconfig\n",
        "\t                            )\n",
        "deploy_step.run_after(select_step)\n",
        "\n",
        "\n",
        "# Run pipeline\n",
        "experiment_name = 'MLOps-Workshop'\n",
        "pipeline = Pipeline(workspace=workspace, steps=[deploy_step])\n",
        "pipeline_run = Experiment(workspace, experiment_name).submit(pipeline)\n",
        "print(\"Pipeline is submitted for execution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "![Published endpoint](./images/endpoint.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Test Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673518832081
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import requests, json\n",
        "\n",
        "# replace uri below with your service endpoint\n",
        "uri = '...'\n",
        "\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "comment1 = \"\"\"\n",
        "\"I've been using Dreamweaver (and it's predecessor Macromedia's UltraDev) for many years.  \n",
        " This is a great tool for someone who is a relative novice at web design.  \n",
        " If you're a novice, a relative newcomer or just an experienced web \n",
        " designer who wants a refresher course, this is a good way to do it.\"\n",
        "\"\"\"\n",
        "sample_input = json.dumps({\n",
        "    'data': [comment1]\n",
        "})\n",
        "response = requests.post(uri, data=sample_input, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673518835544
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "comment2 = \"\"\"\n",
        "\"It is the worst software I have ever used. Very bad UX\"\n",
        "\"\"\"\n",
        "sample_input = json.dumps({\n",
        "    'data': [comment2]\n",
        "})\n",
        "response = requests.post(uri, data=sample_input, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Publish Pipeline\n",
        "\n",
        "- Can be published as a REST endpoint to run om different inputs/ clients (python, C#, Java, ...)\n",
        "- Can be configured to accept parameters \n",
        "- versioned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673459566300
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "     name=\"MLOps-Workshop-Pipeline\",\n",
        "     description=\"Published Pipeline for MLOps Workshop\",\n",
        "     version=\"1.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 6. Schedule Pipeline runs\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-trigger-published-pipeline\n",
        "- Time-based schedules: for routine tasks (monitoring data drift)\n",
        "- Change-based schedules: to react to irregular or unpredictable changes (new data uploaded, old data edited,...) - only blob storage monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673459574094
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
        "\n",
        "## create time-based pipeline\n",
        "# Frequency can be Minute / Hour / Day / Week / Month\n",
        "recurrence = ScheduleRecurrence(frequency=\"Month\", interval=1)\n",
        "recurring_schedule = Schedule.create(workspace, name=\"MonthlySchedule\", \n",
        "                            description=\"Based on time\",\n",
        "                            pipeline_id=published_pipeline.id, \n",
        "                            experiment_name=experiment_name, \n",
        "                            recurrence=recurrence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673459575262
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "recurring_schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Pipeline Schedule Management\n",
        "\n",
        "- Enable / Disable\n",
        "\n",
        "enable(wait_for_provisioning=False, wait_timeout=3600)\n",
        "\n",
        "disable(wait_for_provisioning=False, wait_timeout=3600)\n",
        "\n",
        "- Get (Set the schedule to 'Active' and available to run)\n",
        "\n",
        "get(workspace, id, _workflow_provider=None, _service_endpoint=None)\n",
        "\n",
        "- List ( Get all schedules in the current workspace)\n",
        "\n",
        "list(workspace, active_only=True, pipeline_id=None, pipeline_endpoint_id=None, _workflow_provider=None, _service_endpoint=None)\n",
        "\n",
        "https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
