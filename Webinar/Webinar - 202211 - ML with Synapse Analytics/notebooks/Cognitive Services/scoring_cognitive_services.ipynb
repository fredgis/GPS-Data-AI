{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img width=\"200\"  src=\"https://mmlspark.blob.core.windows.net/graphics/Readme/cog_services_on_spark_2.svg\" />\n",
        "\n",
        "# Cognitive Services\n",
        "\n",
        "[Azure Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/) are a suite of APIs, SDKs, and services available to help developers build intelligent applications without having direct AI or data science skills or knowledge by enabling developers to easily add cognitive features into their applications. The goal of Azure Cognitive Services is to help developers create applications that can see, hear, speak, understand, and even begin to reason. The catalog of services within Azure Cognitive Services can be categorized into five main pillars - Vision, Speech, Language, Web Search, and Decision.\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Vision\n",
        "[**Computer Vision**](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/)\n",
        "- Describe: provides description of an image in human readable language ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/DescribeImage.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.DescribeImage))\n",
        "- Analyze (color, image type, face, adult/racy content): analyzes visual features of an image ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeImage.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeImage))\n",
        "- OCR: reads text from an image ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/OCR.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.OCR))\n",
        "- Recognize Text: reads text from an image ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/RecognizeText.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.RecognizeText))\n",
        "- Thumbnail: generates a thumbnail of user-specified size from the image ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/GenerateThumbnails.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.GenerateThumbnails))\n",
        "- Recognize domain-specific content: recognizes domain-specific content (celebrity, landmark) ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/RecognizeDomainSpecificContent.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.RecognizeDomainSpecificContent))\n",
        "- Tag: identifies list of words that are relevant to the in0put image ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/TagImage.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.TagImage))\n",
        "\n",
        "[**Face**](https://azure.microsoft.com/en-us/services/cognitive-services/face/)\n",
        "- Detect: detects human faces in an image ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/DetectFace.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.DetectFace))\n",
        "- Verify: verifies whether two faces belong to a same person, or a face belongs to a person ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/VerifyFaces.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.VerifyFaces))\n",
        "- Identify: finds the closest matches of the specific query person face from a person group ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/IdentifyFaces.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.IdentifyFaces))\n",
        "- Find similar: finds similar faces to the query face in a face list ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/FindSimilarFace.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.FindSimilarFace))\n",
        "- Group: divides a group of faces into disjoint groups based on similarity ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/GroupFaces.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.GroupFaces))\n",
        "\n",
        "### Speech\n",
        "[**Speech Services**](https://azure.microsoft.com/en-us/services/cognitive-services/speech-services/)\n",
        "- Speech-to-text: transcribes audio streams ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/SpeechToText.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.SpeechToText))\n",
        "- Conversation Transcription: transcribes audio streams into live transcripts with identified speakers. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/ConversationTranscription.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.ConversationTranscription))\n",
        "- Text to Speech: Converts text to realistic audio ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/TextToSpeech.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.TextToSpeech))\n",
        "\n",
        "\n",
        "### Language\n",
        "[**Text Analytics**](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n",
        "- Language detection: detects language of the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/LanguageDetector.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.LanguageDetector))\n",
        "- Key phrase extraction: identifies the key talking points in the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/KeyPhraseExtractor.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.KeyPhraseExtractor))\n",
        "- Named entity recognition: identifies known entities and general named entities in the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/NER.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.NER))\n",
        "- Sentiment analysis: returns a score betwee 0 and 1 indicating the sentiment in the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/TextSentiment.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.TextSentiment))\n",
        "- Healthcare Entity Extraction: Extracts medical entities and relationships from text. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeHealthText.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeHealthText))\n",
        "\n",
        "\n",
        "### Translation\n",
        "[**Translator**](https://azure.microsoft.com/en-us/services/cognitive-services/translator/)\n",
        "- Translate: Translates text. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/Translate.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.Translate))\n",
        "- Transliterate: Converts text in one language from one script to another script. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/Transliterate.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.Transliterate))\n",
        "- Detect: Identifies the language of a piece of text. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/Detect.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.Detect))\n",
        "- BreakSentence: Identifies the positioning of sentence boundaries in a piece of text. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/BreakSentence.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.BreakSentence))\n",
        "- Dictionary Lookup: Provides alternative translations for a word and a small number of idiomatic phrases. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/DictionaryLookup.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.DictionaryLookup))\n",
        "- Dictionary Examples: Provides examples that show how terms in the dictionary are used in context. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/DictionaryExamples.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.DictionaryExamples))\n",
        "- Document Translation: Translates documents across all supported languages and dialects while preserving document structure and data format. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/DocumentTranslator.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.DocumentTranslator))\n",
        "\n",
        "### Form Recognizer\n",
        "[**Form Recognizer**](https://azure.microsoft.com/en-us/services/form-recognizer/)\n",
        "- Analyze Layout: Extract text and layout information from a given document. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeLayout.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeLayout))\n",
        "- Analyze Receipts: Detects and extracts data from receipts using optical character recognition (OCR) and our receipt model, enabling you to easily extract structured data from receipts such as merchant name, merchant phone number, transaction date, transaction total, and more. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeReceipts.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeReceipts))\n",
        "- Analyze Business Cards: Detects and extracts data from business cards using optical character recognition (OCR) and our business card model, enabling you to easily extract structured data from business cards such as contact names, company names, phone numbers, emails, and more. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeBusinessCards.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeBusinessCards))\n",
        "- Analyze Invoices: Detects and extracts data from invoices using optical character recognition (OCR) and our invoice understanding deep learning models, enabling you to easily extract structured data from invoices such as customer, vendor, invoice ID, invoice due date, total, invoice amount due, tax amount, ship to, bill to, line items and more. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeInvoices.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeInvoices))\n",
        "- Analyze ID Documents: Detects and extracts data from identification documents using optical character recognition (OCR) and our ID document model, enabling you to easily extract structured data from ID documents such as first name, last name, date of birth, document number, and more. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeIDDocuments.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeIDDocuments))\n",
        "- Analyze Custom Form: Extracts information from forms (PDFs and images) into structured data based on a model created from a set of representative training forms. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/AnalyzeCustomModel.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AnalyzeCustomModel))\n",
        "- Get Custom Model: Get detailed information about a custom model. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/GetCustomModel.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/ListCustomModels.html))\n",
        "- List Custom Models: Get information about all custom models. ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/ListCustomModels.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.ListCustomModels))\n",
        "\n",
        "### Decision\n",
        "[**Anomaly Detector**](https://azure.microsoft.com/en-us/services/cognitive-services/anomaly-detector/)\n",
        "- Anomaly status of latest point: generates a model using preceding points and determines whether the latest point is anomalous ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/DetectLastAnomaly.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.DetectLastAnomaly))\n",
        "- Find anomalies: generates a model using an entire series and finds anomalies in the series ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/DetectAnomalies.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.DetectAnomalies))\n",
        "\n",
        "### Search\n",
        "- [Bing Image search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/) ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/com/microsoft/azure/synapse/ml/cognitive/BingImageSearch.html), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.BingImageSearch))\n",
        "- [Azure Cognitive search](https://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search) ([Scala](https://mmlspark.blob.core.windows.net/docs/0.10.1/scala/index.html#com.microsoft.azure.synapse.ml.cognitive.AzureSearchWriter$), [Python](https://mmlspark.blob.core.windows.net/docs/0.10.1/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.AzureSearchWriter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Azure Synapse Analytics workspace with an Azure Data Lake Storage Gen2 storage account configured as the default storage. You need to be the Storage Blob Data Contributor of the Data Lake Storage Gen2 file system that you work with.\n",
        "- Spark pool in your Azure Synapse Analytics workspace. For details, see [Create a Spark pool in Azure Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/quickstart-create-sql-pool-studio).\n",
        "- Pre-configuration steps described in the tutorial [Configure Cognitive Services in Azure Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/quickstart-create-sql-pool-studio).\n",
        "\n",
        "<img width=\"400\"  src=\"https://abhishekbaranwal10.files.wordpress.com/2018/09/introduction-to-apache-spark-20-12-638.jpg?resize=638%2C479\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:36:35.2863312Z",
              "execution_start_time": "2022-11-13T21:36:33.3840931Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:35:17.3750587Z",
              "session_id": "7",
              "session_start_time": "2022-11-13T21:35:17.4126144Z",
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import required packages\n",
        "from pyspark.sql.functions import udf, col\n",
        "from synapse.ml.io.http import HTTPTransformer, http_udf\n",
        "from requests import Request\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql.functions import col\n",
        "import os\n",
        "\n",
        "import synapse.ml\n",
        "from synapse.ml.cognitive import *\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:37:17.6693004Z",
              "execution_start_time": "2022-11-13T21:37:10.6096361Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:37:04.667254Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get cognitive services key\n",
        "\n",
        "# A general Cognitive Services key for Text Analytics and Computer Vision (or use separate keys that belong to each service)\n",
        "cognitive_service_key = mssparkutils.credentials.getSecret(\"synapse-ml-demo\", \"cognitive-api-key\",\"AzureKeyVault\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:56:15.5236308Z",
              "execution_start_time": "2022-11-13T21:56:15.3518482Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:56:15.2365755Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 20
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 20, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# set linked servie name\n",
        "cognitive_service_name = \"CognitiveServices\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Analytics sample\n",
        "\n",
        "The [Text Analytics](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/) service provides several algorithms for extracting intelligent insights from text. For example, we can find the sentiment of given input text. The service will return a score between 0.0 and 1.0 where low scores indicate negative sentiment and high score indicates positive sentiment.  This sample uses three simple sentences and returns the sentiment for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:57:28.0268055Z",
              "execution_start_time": "2022-11-13T21:57:26.0727895Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:57:25.9229191Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 23
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 23, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "39052297-1b10-441f-9b4f-205f5c4d79d8",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 39052297-1b10-441f-9b4f-205f5c4d79d8)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a dataframe that's tied to it's column names\n",
        "df = spark.createDataFrame(\n",
        "    [\n",
        "        (\"I am so happy today, its sunny!\", \"en-US\"),\n",
        "        (\"I am frustrated by this rush hour traffic\", \"en-US\"),\n",
        "        (\"The cognitive services on spark aint bad\", \"en-US\"),\n",
        "    ],\n",
        "    [\"text\", \"language\"],\n",
        ")\n",
        "\n",
        "# Run the Text Analytics service with options\n",
        "sentiment = (\n",
        "    TextSentiment()\n",
        "    .setTextCol(\"text\")\n",
        "    .setLocation(\"westus\")\n",
        "    .setSubscriptionKey(cognitive_service_key)\n",
        "    .setOutputCol(\"sentiment\")\n",
        "    .setErrorCol(\"error\")\n",
        "    .setLanguageCol(\"language\")\n",
        ")\n",
        "\n",
        "# Show the results of your text query in a table format\n",
        "display(\n",
        "    sentiment.transform(df).select(\n",
        "        \"text\", col(\"sentiment.document.sentiment\").alias(\"sentiment\")\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Analytics for Health Sample\n",
        "\n",
        "The [Text Analytics for Heatlth Service](https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/text-analytics-for-health/overview?tabs=ner) extracts and labels relevant medical information from unstructured texts such as doctor's notes, discharge summaries, clinical documents, and electronic health records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:40:41.8789378Z",
              "execution_start_time": "2022-11-13T21:40:33.0221455Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:40:28.0654788Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "d1ac825d-db86-4088-a7f8-eb122dc271cc",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, d1ac825d-db86-4088-a7f8-eb122dc271cc)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = spark.createDataFrame(\n",
        "    [\n",
        "        (\"20mg of ibuprofen twice a day\",),\n",
        "        (\"1tsp of Tylenol every 4 hours\",),\n",
        "        (\"6-drops of Vitamin B-12 every evening\",),\n",
        "    ],\n",
        "    [\"text\"],\n",
        ")\n",
        "\n",
        "healthcare = (\n",
        "    AnalyzeHealthText()\n",
        "    .setSubscriptionKey(cognitive_service_key)\n",
        "    .setLocation(\"westus\")\n",
        "    .setLanguage(\"en\")\n",
        "    .setOutputCol(\"response\")\n",
        ")\n",
        "\n",
        "display(healthcare.transform(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translator sample\n",
        "[Translator](https://azure.microsoft.com/en-us/services/cognitive-services/translator/) is a cloud-based machine translation service and is part of the Azure Cognitive Services family of cognitive APIs used to build intelligent apps. Translator is easy to integrate in your applications, websites, tools, and solutions. It allows you to add multi-language user experiences in 90 languages and dialects and can be used for text translation with any operating system. In this sample, we do a simple text translation by providing the sentences you want to translate and target languages you want to translate to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:40:45.3337933Z",
              "execution_start_time": "2022-11-13T21:40:42.0189466Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:40:41.0295384Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "199b1afb-e639-46d2-a3f2-5196b65f6d89",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 199b1afb-e639-46d2-a3f2-5196b65f6d89)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, flatten\n",
        "\n",
        "# Create a dataframe including sentences you want to translate\n",
        "df = spark.createDataFrame(\n",
        "    [([\"Hello, what is your name?\", \"Bye\"],)],\n",
        "    [\n",
        "        \"text\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Run the Translator service with options\n",
        "translate = (\n",
        "    Translate()\n",
        "    .setSubscriptionKey(cognitive_service_key)\n",
        "    .setLocation(\"westus\")\n",
        "    .setTextCol(\"text\")\n",
        "    .setToLanguage([\"zh-Hans\"])\n",
        "    .setOutputCol(\"translation\")\n",
        ")\n",
        "\n",
        "# Show the results of the translation.\n",
        "display(\n",
        "    translate.transform(df)\n",
        "    .withColumn(\"translation\", flatten(col(\"translation.translations\")))\n",
        "    .withColumn(\"translation\", col(\"translation.text\"))\n",
        "    .select(\"translation\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Language detection sample\n",
        "The Language Detector evaluates text input for each document and returns language identifiers with a score that indicates the strength of the analysis. This capability is useful for content stores that collect arbitrary text, where language is unknown. See the [Supported languages](https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/language-support?tabs=language-detection) in Text Analytics API for the list of enabled languages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:46:44.1377151Z",
              "execution_start_time": "2022-11-13T21:46:39.8472143Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:46:39.6442806Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 15
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 15, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "9ee00542-93ab-4894-b36e-07b7187ba1dc",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 9ee00542-93ab-4894-b36e-07b7187ba1dc)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a dataframe that's tied to it's column names\n",
        "df = spark.createDataFrame([\n",
        "  (\"Hello World\",),\n",
        "  (\"Bonjour tout le monde\",),\n",
        "  (\"La carretera estaba atascada. Había mucho tráfico el día de ayer.\",),\n",
        "  (\"你好\",),\n",
        "  (\"こんにちは\",),\n",
        "  (\":) :( :D\",)\n",
        "], [\"text\",])\n",
        "\n",
        "# Run the Text Analytics service with options\n",
        "language = (LanguageDetector()\n",
        "    .setSubscriptionKey(cognitive_service_key)\n",
        "    .setLocation(\"westus\")    .setTextCol(\"text\")\n",
        "    .setOutputCol(\"language\")\n",
        "    .setErrorCol(\"error\"))\n",
        "\n",
        "# Show the results of your text query in a table format\n",
        "display(language.transform(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Form Recognizer sample\n",
        "[Form Recognizer](https://azure.microsoft.com/en-us/services/form-recognizer/) is a part of Azure Applied AI Services that lets you build automated data processing software using machine learning technology. Identify and extract text, key/value pairs, selection marks, tables, and structure from your documents—the service outputs structured data that includes the relationships in the original file, bounding boxes, confidence and more. In this sample, we analyze a business card image and extract its information into structured data.\n",
        "\n",
        "<img width=400 src=\"https://mmlspark.blob.core.windows.net/datasets/FormRecognizer/business_card.jpg\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:41:00.8240554Z",
              "execution_start_time": "2022-11-13T21:40:52.0208006Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:40:51.884566Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 9
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 9, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "f7d62b69-18d6-40a3-b135-a5fab7f8087e",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, f7d62b69-18d6-40a3-b135-a5fab7f8087e)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, explode\n",
        "\n",
        "# Create a dataframe containing the source files\n",
        "imageDf = spark.createDataFrame(\n",
        "    [\n",
        "        (\n",
        "            \"https://mmlspark.blob.core.windows.net/datasets/FormRecognizer/business_card.jpg\",\n",
        "        )\n",
        "    ],\n",
        "    [\n",
        "        \"source\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Run the Form Recognizer service\n",
        "analyzeBusinessCards = (\n",
        "    AnalyzeBusinessCards()\n",
        "    .setSubscriptionKey(cognitive_service_key)\n",
        "    .setLocation(\"westus\")\n",
        "    .setImageUrlCol(\"source\")\n",
        "    .setOutputCol(\"businessCards\")\n",
        ")\n",
        "\n",
        "# Show the results of recognition.\n",
        "display(\n",
        "    analyzeBusinessCards.transform(imageDf)\n",
        "    .withColumn(\n",
        "        \"documents\", explode(col(\"businessCards.analyzeResult.documentResults.fields\"))\n",
        "    )\n",
        "    .select(\"source\", \"documents\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computer Vision sample\n",
        "\n",
        "[Computer Vision](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/) analyzes images to identify structure such as faces, objects, and natural-language descriptions. In this sample, we tag a list of images. Tags are one-word descriptions of things in the image like recognizable objects, people, scenery, and actions.\n",
        "\n",
        "<img width=400 src=\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/objects.jpg\">\n",
        "\n",
        "<img width=400 src=\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/house.jpg\">\n",
        "\n",
        "<img width=400 src=\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/dog.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T22:00:55.297834Z",
              "execution_start_time": "2022-11-13T22:00:52.4285167Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T22:00:52.3039102Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 30
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 30, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "5cf80380-ec2b-4dcf-8fb4-93be078e526f",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 5cf80380-ec2b-4dcf-8fb4-93be078e526f)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a dataframe with the image URLs\n",
        "base_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/\"\n",
        "df = spark.createDataFrame(\n",
        "    [\n",
        "        (base_url + \"objects.jpg\",),\n",
        "        (base_url + \"dog.jpg\",),\n",
        "        (base_url + \"house.jpg\",),\n",
        "    ],\n",
        "    [\n",
        "        \"image\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Run the Computer Vision service. Analyze Image extracts infortmation from/about the images.\n",
        "analysis = (\n",
        "    AnalyzeImage()\n",
        "    .setLocation(\"westus\")\n",
        "    .setSubscriptionKey(cognitive_service_key)\n",
        "    .setVisualFeatures(\n",
        "        [\"Categories\", \"Color\", \"Description\", \"Faces\", \"Objects\", \"Tags\"]\n",
        "    )\n",
        "    .setOutputCol(\"analysis_results\")\n",
        "    .setImageUrlCol(\"image\")\n",
        "    .setErrorCol(\"error\")\n",
        ")\n",
        "\n",
        "# Show the results of what you wanted to pull out of the images.\n",
        "display(analysis.transform(df).select(\"image\", \"analysis_results.description.tags\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Describe image\n",
        "Generate a description of an entire image in human-readable language, using complete sentences. Computer Vision's algorithms generate various descriptions based on the objects identified in the image. The descriptions are each evaluated and a confidence score generated. A list is then returned ordered from highest confidence score to lowest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T22:01:20.767407Z",
              "execution_start_time": "2022-11-13T22:01:18.8708586Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T22:01:18.7536358Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 32
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 32, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "8ea2eb19-9c50-4c88-84c5-210afb2e145a",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 8ea2eb19-9c50-4c88-84c5-210afb2e145a)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "di = (DescribeImage()\n",
        "    .setLocation(\"westus\")\n",
        "    .setSubscriptionKey(cognitive_service_key)\n",
        "    .setMaxCandidates(3)\n",
        "    .setImageUrlCol(\"image\")\n",
        "    .setOutputCol(\"descriptions\"))\n",
        "\n",
        "display(di.transform(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Speech-to-Text sample\n",
        "The [Speech-to-text](https://azure.microsoft.com/en-us/services/cognitive-services/speech-services/) service converts streams or files of spoken audio to text. In this sample, we transcribe one audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-11-13T21:55:25.3083413Z",
              "execution_start_time": "2022-11-13T21:55:06.1140115Z",
              "livy_statement_state": "available",
              "queued_time": "2022-11-13T21:55:05.99444Z",
              "session_id": "7",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkezi1n72",
              "state": "finished",
              "statement_id": 19
            },
            "text/plain": [
              "StatementMeta(sparkezi1n72, 7, 19, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "84680d31-1c92-48d8-a06e-0890b80e8ed4",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 84680d31-1c92-48d8-a06e-0890b80e8ed4)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a dataframe with our audio URLs, tied to the column called \"url\"\n",
        "df = spark.createDataFrame(\n",
        "    [(\"https://mmlspark.blob.core.windows.net/datasets/Speech/audio2.wav\",)], [\"url\"]\n",
        ")\n",
        "\n",
        "# Run the Speech-to-text service to translate the audio into text\n",
        "speech_to_text = (\n",
        "    SpeechToTextSDK()\n",
        "    .setLinkedService(\"CognitiveServices\")\n",
        "    .setOutputCol(\"text\")\n",
        "    .setAudioDataCol(\"url\")\n",
        "    .setLanguage(\"en-US\")\n",
        "    .setProfanity(\"Masked\")\n",
        ")\n",
        "\n",
        "# Show the results of the translation\n",
        "display(speech_to_text.transform(df).select(\"url\", \"text.DisplayText\"))"
      ]
    }
  ],
  "metadata": {
    "description": "Overview of cognitive services\n",
    "kernelspec": {
      "display_name": "Python 3.9.7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "199b1afb-e639-46d2-a3f2-5196b65f6d89": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": [
                    "你好，你叫什么名字？",
                    "再见"
                  ]
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "translation",
                  "type": "ArrayType(StringType,true)"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        },
        "39052297-1b10-441f-9b4f-205f5c4d79d8": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "I am so happy today, its sunny!",
                  "1": "positive"
                },
                {
                  "0": "I am frustrated by this rush hour traffic",
                  "1": "negative"
                },
                {
                  "0": "The cognitive services on spark aint bad",
                  "1": "negative"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "text",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "sentiment",
                  "type": "string"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        },
        "5cf80380-ec2b-4dcf-8fb4-93be078e526f": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/objects.jpg",
                  "1": [
                    "skating",
                    "person",
                    "man",
                    "outdoor",
                    "riding",
                    "sport",
                    "skateboard",
                    "young",
                    "board",
                    "shirt",
                    "air",
                    "park",
                    "boy",
                    "side",
                    "jumping",
                    "ramp",
                    "trick",
                    "doing",
                    "flying"
                  ]
                },
                {
                  "0": "https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/dog.jpg",
                  "1": [
                    "dog",
                    "outdoor",
                    "fence",
                    "wooden",
                    "small",
                    "brown",
                    "building",
                    "sitting",
                    "front",
                    "bench",
                    "standing",
                    "table",
                    "walking",
                    "board",
                    "beach",
                    "holding",
                    "bridge"
                  ]
                },
                {
                  "0": "https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/house.jpg",
                  "1": [
                    "outdoor",
                    "grass",
                    "house",
                    "building",
                    "old",
                    "home",
                    "front",
                    "small",
                    "church",
                    "stone",
                    "large",
                    "grazing",
                    "yard",
                    "green",
                    "sitting",
                    "leading",
                    "sheep",
                    "brick",
                    "bench",
                    "street",
                    "country",
                    "clock",
                    "sign",
                    "parked",
                    "field",
                    "standing",
                    "garden",
                    "water",
                    "horse",
                    "man",
                    "tall",
                    "fire",
                    "group"
                  ]
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "image",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "tags",
                  "type": "ArrayType(StringType,true)"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        },
        "84680d31-1c92-48d8-a06e-0890b80e8ed4": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "https://mmlspark.blob.core.windows.net/datasets/Speech/audio2.wav",
                  "1": "Custom speech provides tools that allow you to visually inspect the recognition quality of a model. By comparing audio data with the corresponding recognition result from the custom speech portal, you can playback uploaded audio and determine if the provided recognition result is correct. This tool allows you to quickly inspect quality of Microsoft's baseline speech to text model or a trained custom model without having to transcribe any audio data."
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "url",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "DisplayText",
                  "type": "string"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        },
        "8ea2eb19-9c50-4c88-84c5-210afb2e145a": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/objects.jpg",
                  "2": {
                    "description": {
                      "captions": [
                        {
                          "confidence": 0.8022720663054775,
                          "text": "a young man riding a skateboard"
                        },
                        {
                          "confidence": 0.8012720663054775,
                          "text": "a man riding a skateboard"
                        },
                        {
                          "confidence": 0.8002720663054775,
                          "text": "a young man riding a skateboard down the side of a ramp"
                        }
                      ],
                      "tags": [
                        "skating",
                        "person",
                        "man",
                        "outdoor",
                        "riding",
                        "sport",
                        "skateboard",
                        "young",
                        "board",
                        "shirt",
                        "air",
                        "park",
                        "boy",
                        "side",
                        "jumping",
                        "ramp",
                        "trick",
                        "doing",
                        "flying"
                      ]
                    },
                    "metadata": {
                      "format": "Jpeg",
                      "height": 462,
                      "width": 600
                    }
                  }
                },
                {
                  "0": "https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/dog.jpg",
                  "2": {
                    "description": {
                      "captions": [
                        {
                          "confidence": 0.6623354474017079,
                          "text": "a dog standing on top of a wooden fence"
                        },
                        {
                          "confidence": 0.5826920349579183,
                          "text": "a dog sitting on top of a wooden fence"
                        },
                        {
                          "confidence": 0.5816920349579183,
                          "text": "a dog walking on top of a wooden fence"
                        }
                      ],
                      "tags": [
                        "dog",
                        "outdoor",
                        "fence",
                        "wooden",
                        "small",
                        "brown",
                        "building",
                        "sitting",
                        "front",
                        "bench",
                        "standing",
                        "table",
                        "walking",
                        "board",
                        "beach",
                        "holding",
                        "bridge"
                      ]
                    },
                    "metadata": {
                      "format": "Jpeg",
                      "height": 450,
                      "width": 600
                    }
                  }
                },
                {
                  "0": "https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/house.jpg",
                  "2": {
                    "description": {
                      "captions": [
                        {
                          "confidence": 0.978387190537345,
                          "text": "a house with trees in the background"
                        },
                        {
                          "confidence": 0.9741256903916443,
                          "text": "an old house with trees in the background"
                        },
                        {
                          "confidence": 0.9720384540450293,
                          "text": "a small house in the background"
                        }
                      ],
                      "tags": [
                        "outdoor",
                        "grass",
                        "house",
                        "building",
                        "old",
                        "home",
                        "front",
                        "small",
                        "church",
                        "stone",
                        "large",
                        "grazing",
                        "yard",
                        "green",
                        "sitting",
                        "leading",
                        "sheep",
                        "brick",
                        "bench",
                        "street",
                        "country",
                        "clock",
                        "sign",
                        "parked",
                        "field",
                        "standing",
                        "garden",
                        "water",
                        "horse",
                        "man",
                        "tall",
                        "fire",
                        "group"
                      ]
                    },
                    "metadata": {
                      "format": "Jpeg",
                      "height": 480,
                      "width": 640
                    }
                  }
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "image",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "DescribeImage_3e3ac4bf40b0_error",
                  "type": "StructType(StructField(response,StringType,true), StructField(status,StructType(StructField(protocolVersion,StructType(StructField(protocol,StringType,true), StructField(major,IntegerType,false), StructField(minor,IntegerType,false)),true), StructField(statusCode,IntegerType,false), StructField(reasonPhrase,StringType,true)),true))"
                },
                {
                  "key": "2",
                  "name": "descriptions",
                  "type": "StructType(StructField(description,StructType(StructField(tags,ArrayType(StringType,true),true), StructField(captions,ArrayType(StructType(StructField(text,StringType,true), StructField(confidence,DoubleType,true)),true),true)),true), StructField(requestID,StringType,true), StructField(metadata,StructType(StructField(width,IntegerType,true), StructField(height,IntegerType,true), StructField(format,StringType,true)),true))"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        },
        "9ee00542-93ab-4894-b36e-07b7187ba1dc": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "Hello World",
                  "2": {
                    "document": {
                      "detectedLanguage": {
                        "confidenceScore": 1,
                        "iso6391Name": "en",
                        "name": "English"
                      },
                      "id": "0",
                      "warnings": []
                    },
                    "modelVersion": "2021-11-20"
                  }
                },
                {
                  "0": "Bonjour tout le monde",
                  "2": {
                    "document": {
                      "detectedLanguage": {
                        "confidenceScore": 0.88,
                        "iso6391Name": "fr",
                        "name": "French"
                      },
                      "id": "0",
                      "warnings": []
                    },
                    "modelVersion": "2021-11-20"
                  }
                },
                {
                  "0": "La carretera estaba atascada. Había mucho tráfico el día de ayer.",
                  "2": {
                    "document": {
                      "detectedLanguage": {
                        "confidenceScore": 1,
                        "iso6391Name": "es",
                        "name": "Spanish"
                      },
                      "id": "0",
                      "warnings": []
                    },
                    "modelVersion": "2021-11-20"
                  }
                },
                {
                  "0": "你好",
                  "2": {
                    "document": {
                      "detectedLanguage": {
                        "confidenceScore": 1,
                        "iso6391Name": "zh",
                        "name": "Chinese"
                      },
                      "id": "0",
                      "warnings": []
                    },
                    "modelVersion": "2021-11-20"
                  }
                },
                {
                  "0": "こんにちは",
                  "2": {
                    "document": {
                      "detectedLanguage": {
                        "confidenceScore": 1,
                        "iso6391Name": "ja",
                        "name": "Japanese"
                      },
                      "id": "0",
                      "warnings": []
                    },
                    "modelVersion": "2021-11-20"
                  }
                },
                {
                  "0": ":) :( :D",
                  "2": {
                    "document": {
                      "detectedLanguage": {
                        "confidenceScore": 0,
                        "iso6391Name": "(Unknown)",
                        "name": "(Unknown)"
                      },
                      "id": "0",
                      "warnings": []
                    },
                    "modelVersion": "2021-11-20"
                  }
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "text",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "error",
                  "type": "StructType(StructField(response,StringType,true), StructField(status,StructType(StructField(protocolVersion,StructType(StructField(protocol,StringType,true), StructField(major,IntegerType,false), StructField(minor,IntegerType,false)),true), StructField(statusCode,IntegerType,false), StructField(reasonPhrase,StringType,true)),true))"
                },
                {
                  "key": "2",
                  "name": "language",
                  "type": "StructType(StructField(statistics,StructType(StructField(documentsCount,IntegerType,false), StructField(validDocumentsCount,IntegerType,false), StructField(erroneousDocumentsCount,IntegerType,false), StructField(transactionsCount,IntegerType,false)),true), StructField(document,StructType(StructField(id,StringType,true), StructField(detectedLanguage,StructType(StructField(name,StringType,true), StructField(iso6391Name,StringType,true), StructField(confidenceScore,DoubleType,false)),true), StructField(warnings,ArrayType(StructType(StructField(code,StringType,true), StructField(message,StringType,true), StructField(targetRef,StringType,true)),true),true), StructField(statistics,StructType(StructField(charactersCount,IntegerType,false), StructField(transactionsCount,IntegerType,false)),true)),true), StructField(error,StructType(StructField(id,StringType,true), StructField(error,StringType,true)),true), StructField(modelVersion,StringType,true))"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        },
        "d1ac825d-db86-4088-a7f8-eb122dc271cc": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "20mg of ibuprofen twice a day",
                  "2": {
                    "document": {
                      "entities": [
                        {
                          "category": "Dosage",
                          "confidenceScore": 0.99,
                          "length": 4,
                          "offset": 0,
                          "text": "20mg"
                        },
                        {
                          "category": "MedicationName",
                          "confidenceScore": 1,
                          "length": 9,
                          "offset": 8,
                          "text": "ibuprofen"
                        },
                        {
                          "category": "Frequency",
                          "confidenceScore": 1,
                          "length": 11,
                          "offset": 18,
                          "text": "twice a day"
                        }
                      ],
                      "id": "0",
                      "relations": [
                        {
                          "entities": [
                            {
                              "ref": "#/results/documents/0/entities/0",
                              "role": "Dosage"
                            },
                            {
                              "ref": "#/results/documents/0/entities/1",
                              "role": "Medication"
                            }
                          ],
                          "relationType": "DosageOfMedication"
                        },
                        {
                          "entities": [
                            {
                              "ref": "#/results/documents/0/entities/1",
                              "role": "Medication"
                            },
                            {
                              "ref": "#/results/documents/0/entities/2",
                              "role": "Frequency"
                            }
                          ],
                          "relationType": "FrequencyOfMedication"
                        }
                      ],
                      "warnings": []
                    },
                    "modelVersion": "2022-03-01"
                  }
                },
                {
                  "0": "1tsp of Tylenol every 4 hours",
                  "2": {
                    "document": {
                      "entities": [
                        {
                          "category": "Dosage",
                          "confidenceScore": 1,
                          "length": 4,
                          "offset": 0,
                          "text": "1tsp"
                        },
                        {
                          "category": "MedicationName",
                          "confidenceScore": 1,
                          "length": 7,
                          "offset": 8,
                          "text": "Tylenol"
                        },
                        {
                          "category": "Frequency",
                          "confidenceScore": 1,
                          "length": 13,
                          "offset": 16,
                          "text": "every 4 hours"
                        }
                      ],
                      "id": "0",
                      "relations": [
                        {
                          "entities": [
                            {
                              "ref": "#/results/documents/0/entities/0",
                              "role": "Dosage"
                            },
                            {
                              "ref": "#/results/documents/0/entities/1",
                              "role": "Medication"
                            }
                          ],
                          "relationType": "DosageOfMedication"
                        },
                        {
                          "entities": [
                            {
                              "ref": "#/results/documents/0/entities/1",
                              "role": "Medication"
                            },
                            {
                              "ref": "#/results/documents/0/entities/2",
                              "role": "Frequency"
                            }
                          ],
                          "relationType": "FrequencyOfMedication"
                        }
                      ],
                      "warnings": []
                    },
                    "modelVersion": "2022-03-01"
                  }
                },
                {
                  "0": "6-drops of Vitamin B-12 every evening",
                  "2": {
                    "document": {
                      "entities": [
                        {
                          "category": "Dosage",
                          "confidenceScore": 0.97,
                          "length": 7,
                          "offset": 0,
                          "text": "6-drops"
                        },
                        {
                          "category": "MedicationName",
                          "confidenceScore": 0.99,
                          "length": 12,
                          "offset": 11,
                          "text": "Vitamin B-12"
                        },
                        {
                          "category": "Frequency",
                          "confidenceScore": 1,
                          "length": 13,
                          "offset": 24,
                          "text": "every evening"
                        }
                      ],
                      "id": "0",
                      "relations": [
                        {
                          "entities": [
                            {
                              "ref": "#/results/documents/0/entities/0",
                              "role": "Dosage"
                            },
                            {
                              "ref": "#/results/documents/0/entities/1",
                              "role": "Medication"
                            }
                          ],
                          "relationType": "DosageOfMedication"
                        },
                        {
                          "entities": [
                            {
                              "ref": "#/results/documents/0/entities/1",
                              "role": "Medication"
                            },
                            {
                              "ref": "#/results/documents/0/entities/2",
                              "role": "Frequency"
                            }
                          ],
                          "relationType": "FrequencyOfMedication"
                        }
                      ],
                      "warnings": []
                    },
                    "modelVersion": "2022-03-01"
                  }
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "text",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "AnalyzeHealthText_55436a22b0ee_error",
                  "type": "StructType(StructField(response,StringType,true), StructField(status,StructType(StructField(protocolVersion,StructType(StructField(protocol,StringType,true), StructField(major,IntegerType,false), StructField(minor,IntegerType,false)),true), StructField(statusCode,IntegerType,false), StructField(reasonPhrase,StringType,true)),true))"
                },
                {
                  "key": "2",
                  "name": "response",
                  "type": "StructType(StructField(statistics,StructType(StructField(documentsCount,IntegerType,false), StructField(validDocumentsCount,IntegerType,false), StructField(erroneousDocumentsCount,IntegerType,false), StructField(transactionsCount,IntegerType,false)),true), StructField(document,StructType(StructField(id,StringType,true), StructField(entities,ArrayType(StructType(StructField(offset,IntegerType,false), StructField(length,IntegerType,false), StructField(text,StringType,true), StructField(category,StringType,true), StructField(confidenceScore,DoubleType,false)),true),true), StructField(relations,ArrayType(StructType(StructField(relationType,StringType,true), StructField(entities,ArrayType(StructType(StructField(ref,StringType,true), StructField(role,StringType,true)),true),true)),true),true), StructField(warnings,ArrayType(StructType(StructField(code,StringType,true), StructField(message,StringType,true), StructField(targetRef,StringType,true)),true),true), StructField(statistics,StructType(StructField(charactersCount,IntegerType,false), StructField(transactionsCount,IntegerType,false)),true)),true), StructField(error,StructType(StructField(id,StringType,true), StructField(error,StringType,true)),true), StructField(modelVersion,StringType,true))"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        },
        "f7d62b69-18d6-40a3-b135-a5fab7f8087e": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "https://mmlspark.blob.core.windows.net/datasets/FormRecognizer/business_card.jpg",
                  "1": {
                    "Addresses": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"string\",\"valueString\":\"2 Kingdom Street Paddington, London, W2 6BD\",\"text\":\"2 Kingdom Street Paddington, London, W2 6BD\",\"boundingBox\":[1227.3,2138.3,2520.2,1690.5,2597,1912.3,1304.1,2360.1],\"page\":1,\"confidence\":0.98}"
                      ]
                    },
                    "CompanyNames": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"string\",\"valueString\":\"Contoso\",\"text\":\"Contoso\",\"boundingBox\":[1146,1926,2224,1587,2285,1756,1202,2099],\"page\":1,\"confidence\":0.168}"
                      ]
                    },
                    "ContactNames": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"object\",\"valueObject\":{\"FirstName\":{\"type\":\"string\",\"valueString\":\"Avery\",\"text\":\"Avery\",\"boundingBox\":[666,1102,1111,995,1140,1114,696,1216],\"page\":1},\"LastName\":{\"type\":\"string\",\"valueString\":\"Smith\",\"text\":\"Smith\",\"boundingBox\":[1170,981,1571,881,1600,1000,1200,1100],\"page\":1}},\"text\":\"Dr. Avery Smith\",\"boundingBox\":[414.1,1153,1572,880.8,1601.3,1005.6,443.5,1277.9],\"page\":1,\"confidence\":0.979}"
                      ]
                    },
                    "Departments": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"string\",\"valueString\":\"Cloud & Al Department\",\"text\":\"Cloud & Al Department\",\"boundingBox\":[473.6,1409.5,1586,1134,1607.3,1219.8,494.9,1495.3],\"page\":1,\"confidence\":0.989}"
                      ]
                    },
                    "Emails": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"string\",\"valueString\":\"avery.smith@contoso.com\",\"text\":\"avery.smith@contoso.com\",\"boundingBox\":[2106,934,2908,706,2921,769,2122,993],\"page\":1,\"confidence\":0.99}"
                      ]
                    },
                    "Faxes": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"phoneNumber\",\"text\":\"+44 (0) 20 6789 2345\",\"boundingBox\":[2520.4,1195.5,3191.8,983.5,3215.8,1059.2,2544.3,1271.3],\"page\":1,\"confidence\":0.99}"
                      ]
                    },
                    "JobTitles": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"string\",\"valueString\":\"Senior Researcher\",\"text\":\"Senior Researcher\",\"boundingBox\":[451,1312,1315.9,1103.6,1334.7,1181.7,469.8,1390],\"page\":1,\"confidence\":0.99}"
                      ]
                    },
                    "MobilePhones": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"phoneNumber\",\"text\":\"+44 (0) 7911 123456\",\"boundingBox\":[2426.8,1040.1,3065.5,846,3087.2,917.6,2448.5,1111.7],\"page\":1,\"confidence\":0.99}"
                      ]
                    },
                    "Websites": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"string\",\"valueString\":\"https://www.contoso.com/\",\"text\":\"https://www.contoso.com/\",\"boundingBox\":[2120,1003,2981,757,3006,824,2139,1075],\"page\":1,\"confidence\":0.99}"
                      ]
                    },
                    "WorkPhones": {
                      "type": "array",
                      "valueArray": [
                        "{\"type\":\"phoneNumber\",\"text\":\"+44 (0) 20 9876 5432\",\"boundingBox\":[2468.2,1118.1,3130.3,914.6,3152.9,988.2,2490.8,1191.6],\"page\":1,\"confidence\":0.989}"
                      ]
                    }
                  }
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "source",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "documents",
                  "type": "MapType(StringType,StructType(StructField(type,StringType,true), StructField(page,IntegerType,true), StructField(confidence,DoubleType,true), StructField(boundingBox,ArrayType(DoubleType,true),true), StructField(text,StringType,true), StructField(valueString,StringType,true), StructField(valuePhoneNumber,StringType,true), StructField(valueNumber,DoubleType,true), StructField(valueDate,StringType,true), StructField(valueTime,StringType,true), StructField(valueObject,StringType,true), StructField(valueArray,ArrayType(StringType,true),true)),true)"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "efae42f5bb22c941f90033c79b5834eea70e75e2a45a38b301087dc593077f92"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
