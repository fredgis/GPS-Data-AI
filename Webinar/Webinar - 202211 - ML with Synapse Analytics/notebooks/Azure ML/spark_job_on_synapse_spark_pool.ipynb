{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved. \n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Synapse Spark Pool as a Compute Target from Azure Machine Learning Remote Run\n",
        "1. To use Synapse Spark Pool as a compute target from Experiment Run, [ScriptRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run_config.scriptrunconfig?view=azure-ml-py) is used, the same as other Experiment Runs. This notebook demonstrates how to leverage ScriptRunConfig to submit an experiment run to an attached Synapse Spark cluster.\n",
        "2. To use Synapse Spark Pool as a compute target from [Azure Machine Learning Pipeline](https://aka.ms/pl-concept), a [SynapseSparkStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.synapse_spark_step.synapsesparkstep?view=azure-ml-py) is used. This notebook demonstrates how to leverage SynapseSparkStep in Azure Machine Learning Pipeline.\n",
        "\n",
        "## Before you begin:\n",
        "1. **Create an Azure Synapse workspace**, check [this] (https://docs.microsoft.com/en-us/azure/synapse-analytics/quickstart-create-workspace) for more information.\n",
        "2. **Create Spark Pool in Synapse workspace**: check [this] (https://docs.microsoft.com/en-us/azure/synapse-analytics/quickstart-create-apache-spark-pool-portal) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Machine Learning and Pipeline SDK-specific imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668600625180
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Experiment\n",
        "from azureml.core import LinkedService, SynapseWorkspaceLinkedServiceConfiguration\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute, SynapseCompute\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "from azureml.data import HDFSOutputDatasetConfig\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep, SynapseSparkStep\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668638742566
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Link Synapse workspace to AML \n",
        "You have to be an \"Owner\" of Synapse workspace resource to perform linking. You can check your role in the Azure resource management portal, if you don't have an \"Owner\" role, you can contact an \"Owner\" to link the workspaces for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668638746598
        }
      },
      "outputs": [],
      "source": [
        "# Replace with your resource info before running.\n",
        "\n",
        "synapse_subscription_id=os.getenv(\"SYNAPSE_SUBSCRIPTION_ID\", \".....\")\n",
        "synapse_resource_group=os.getenv(\"SYNAPSE_RESOURCE_GROUP\", \".....\")\n",
        "synapse_workspace_name=os.getenv(\"SYNAPSE_WORKSPACE_NAME\", \".....\")\n",
        "synapse_linked_service_name=os.getenv(\"SYNAPSE_LINKED_SERVICE_NAME\", \".....\")\n",
        "\n",
        "synapse_link_config = SynapseWorkspaceLinkedServiceConfiguration(\n",
        "    subscription_id=synapse_subscription_id,\n",
        "    resource_group=synapse_resource_group,\n",
        "    name=synapse_workspace_name\n",
        ")\n",
        "\n",
        "linked_service = LinkedService.register(\n",
        "    workspace=ws,\n",
        "    name=synapse_linked_service_name,\n",
        "    linked_service_config=synapse_link_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linked service property\n",
        "\n",
        "A MSI (system_assigned_identity_principal_id) will be generated for each linked service, for example:\n",
        "\n",
        "#### Make sure you grant \"Synapse Apache Spark Administrator\" role of the synapse workspace to the generated workspace linking MSI in Synapse studio portal before you submit job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668638747840
        }
      },
      "outputs": [],
      "source": [
        "linked_service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attach Synapse spark pool as AML compute target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668638751393
        }
      },
      "outputs": [],
      "source": [
        "synapse_spark_pool_name=os.getenv(\"SYNAPSE_SPARK_POOL_NAME\", \".....\")\n",
        "synapse_compute_name=os.getenv(\"SYNAPSE_COMPUTE_NAME\", \".....\")\n",
        "\n",
        "attach_config = SynapseCompute.attach_configuration(\n",
        "        linked_service,\n",
        "        type=\"SynapseSpark\",\n",
        "        pool_name=synapse_spark_pool_name)\n",
        "\n",
        "synapse_compute=ComputeTarget.attach(\n",
        "        workspace=ws,\n",
        "        name=synapse_compute_name,\n",
        "        attach_configuration=attach_config)\n",
        "\n",
        "synapse_compute.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Start an experiment run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668638754675
        }
      },
      "outputs": [],
      "source": [
        "# Use the default blob storage\n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print('Datastore {} will be used'.format(def_blob_store.name))\n",
        "\n",
        "# We are uploading a sample file in the local directory to be used as a datasource\n",
        "file_name = \"Titanic.csv\"\n",
        "def_blob_store.upload_files(files=[\"./{}\".format(file_name)], overwrite=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File dataset as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668638759970
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Dataset\n",
        "titanic_file_dataset = Dataset.File.from_files(path=[(def_blob_store, file_name)])\n",
        "input = titanic_file_dataset.as_named_input(\"file_input\").as_hdfs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output config: the output will be registered as a File dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668638774466
        }
      },
      "outputs": [],
      "source": [
        "from azureml.data import HDFSOutputDatasetConfig\n",
        "output = HDFSOutputDatasetConfig(destination=(def_blob_store,\"prepared-dataset\")).register_on_complete(name=\"titanic-final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataprep script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668602291505
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./Titanic.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668604768845
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"code\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668600365451
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile code/dataprep.py\n",
        "import os\n",
        "import sys\n",
        "import azureml.core\n",
        "from azureml.core import Run, Dataset\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import  *\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "print(azureml.core.VERSION)\n",
        "print(os.environ)\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--file_input\")\n",
        "parser.add_argument(\"--output_dir\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "# use dataset sdk to read tabular dataset\n",
        "#run_context = Run.get_context()\n",
        "#dataset = Dataset.get_by_id(run_context.experiment.workspace,id=args.tabular_input)\n",
        "#sdf = dataset.to_spark_dataframe()\n",
        "#sdf.show(5)\n",
        "\n",
        "\n",
        "# use hdfs path to read file dataset\n",
        "spark= SparkSession.builder.getOrCreate()\n",
        "\n",
        "sdf = spark.read.option(\"header\", \"true\").csv(args.file_input)\n",
        "print(\"Raw df:\")\n",
        "sdf.show(5)\n",
        "\n",
        "# check data schema\n",
        "sdf.printSchema()\n",
        "\n",
        "# statistical summary\n",
        "sdf.describe().show()\n",
        "\n",
        "# Preprocessing\n",
        "## 1- create new column with title\n",
        "sdf = sdf.withColumn('Title',F.regexp_extract(F.col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
        "\n",
        "titles_map = {\n",
        " 'Capt' : 'Rare',\n",
        " 'Col' : 'Rare',\n",
        " 'Don': 'Rare',\n",
        " 'Dona': 'Rare',\n",
        " 'Dr' : 'Rare',\n",
        " 'Jonkheer' :'Rare' ,\n",
        " 'Lady': 'Rare',\n",
        " 'Major': 'Rare',\n",
        " 'Master': 'Master',\n",
        " 'Miss' : 'Miss',\n",
        " 'Mlle' : 'Rare',\n",
        " 'Mme': 'Rare',\n",
        " 'Mr': 'Mr',\n",
        " 'Mrs': 'Mrs',\n",
        " 'Ms': 'Rare',\n",
        " 'Rev': 'Rare',\n",
        " 'Sir': 'Rare',\n",
        " 'Countess': 'Rare'\n",
        "}\n",
        "def impute_title(title):\n",
        "    return titles_map[title]\n",
        "title_map_func = F.udf(lambda x: impute_title(x), StringType())\n",
        "sdf = sdf.withColumn('Title', title_map_func('Title'))\n",
        "\n",
        "## 2- Fill missing values with mean/mode\n",
        "print(\"Filling missing values\")\n",
        "sdf = sdf.withColumn(\"Age\", F.col(\"Age\").cast(IntegerType()))\n",
        "age_mean = round(sdf.select(F.mean(\"Age\")).collect()[0][0])\n",
        "print(\"age_mean: \", age_mean)\n",
        "sdf = sdf.fillna(age_mean, subset=['Age'])\n",
        "\n",
        "embarked_mode = sdf.groupby(\"Embarked\").count().orderBy(\"Embarked\", ascending=False).first()[0]\n",
        "sdf = sdf.fillna(embarked_mode, subset=['Embarked'])\n",
        "\n",
        "## 3- Replace cabin to 1st char from the string\n",
        "print(\"Modifying cabin column\")\n",
        "sdf = sdf.withColumn(\"Cabin\", sdf.Cabin.substr(0, 1))\n",
        "### fill null values with U - undefined\n",
        "sdf = sdf.fillna('U', subset=['Cabin'])\n",
        "\n",
        "## 4- create family size column from Parch and SibSp \n",
        "print(\"Creating family column\")\n",
        "sdf = sdf.withColumn('Family_size', F.col('Parch')+ F.col('SibSp'))\n",
        "\n",
        "## 5- transform string columns to int values\n",
        "print(\"Indexing strings\")\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(sdf) for column in [\"Sex\",\"Embarked\",\"Title\", \"Cabin\"]]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "sdf = pipeline.fit(sdf).transform(sdf)\n",
        "\n",
        "## 6- Drop unecessary columns\n",
        "sdf = sdf.drop('Sex','PassengerId','Name','Title', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked')\n",
        "\n",
        "print(\"Preprocessed df:\")\n",
        "sdf.show(5)\n",
        "sdf.printSchema()\n",
        "sdf.describe().show()\n",
        "\n",
        "# Colaesce results and write back to csv format\n",
        "sdf.coalesce(1).write\\\n",
        ".option(\"header\", \"true\")\\\n",
        ".mode(\"append\")\\\n",
        ".csv(args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up Conda dependency for the following Script Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668614611374
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.environment import CondaDependencies\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-core==1.20.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to leverage ScriptRunConfig to submit an experiment run to an attached Synapse Spark cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668614623034
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import RunConfiguration\n",
        "from azureml.core import ScriptRunConfig \n",
        "from azureml.core import Experiment\n",
        "\n",
        "run_config = RunConfiguration(framework=\"pyspark\")\n",
        "run_config.target = synapse_compute_name\n",
        "\n",
        "run_config.spark.configuration[\"spark.driver.memory\"] = \"32g\" \n",
        "run_config.spark.configuration[\"spark.driver.cores\"] = 2\n",
        "run_config.spark.configuration[\"spark.executor.memory\"] = \"32g\" \n",
        "run_config.spark.configuration[\"spark.executor.cores\"] = 2\n",
        "run_config.spark.configuration[\"spark.executor.instances\"] = 2 \n",
        "\n",
        "run_config.environment.python.conda_dependencies = conda_dep\n",
        "\n",
        "script_run_config = ScriptRunConfig(source_directory = './code',\n",
        "                                    script= 'dataprep.py',\n",
        "                                    arguments = [\"--file_input\", input,\n",
        "                                                 \"--output_dir\", output],\n",
        "                                    run_config = run_config) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668614630585
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment \n",
        "exp = Experiment(workspace=ws, name=\"synapse-spark\") \n",
        "run = exp.submit(config=script_run_config) \n",
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to leverage SynapseSparkStep in an AML pipeline to orchestrate data prep step on Synapse Spark and training step on AzureML compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668606529175
        }
      },
      "outputs": [],
      "source": [
        "# Choose a name for your CPU cluster\n",
        "cpu_cluster_name = \"cpucluster\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',\n",
        "                                                           min_nodes=0,\n",
        "                                                           max_nodes=3)\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
        "\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile code/train_2.py\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from azureml.core import Run\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "mypath = os.environ[\"step2_input\"]\n",
        "csv_files = [f for f in listdir(\"mypath\") if isfile(join(\"mypath\", f)) and f.startswith('part-00000')]\n",
        "\n",
        "# start an Azure ML run\n",
        "run = Run.get_context()\n",
        "\n",
        "df = pd.DataFrame()\n",
        "#append all files together\n",
        "for file in csv_files:\n",
        "            df_temp = pd.read_csv(file)\n",
        "            df = df.append(df_temp, ignore_index=True)\n",
        "            \n",
        "df=df.dropna()\n",
        "df.head()\n",
        "\n",
        "X = df.drop(['Survived'],axis=1)\n",
        "y = df.Survived\n",
        "# split train test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)\n",
        "\n",
        "# train the model\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
        "# Add labels to your graph\n",
        "plt.xlabel('Feature Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.title(\"Visualizing Important Features\")\n",
        "plt.tight_layout()\n",
        "run.log_image(name='feature_importance.png', plot=plt)\n",
        "\n",
        "# Get accuracy + Confusion matrix\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {}\".format(accuracy))\n",
        "run.log('Accuracy', np.float(accuracy))\n",
        "\n",
        "print(classification_report(y_test,y_pred))\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.heatmap(conf_matrix, annot=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "run.log_image(name='confusion_matrix.png', plot=plt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile code/train.py\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "mypath = os.environ[\"step2_input\"]\n",
        "files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "for file in files:\n",
        "    with open(join(mypath,file)) as f:\n",
        "        print(f.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668618716529
        }
      },
      "outputs": [],
      "source": [
        "titanic_tabular_dataset = Dataset.Tabular.from_delimited_files(path=[(def_blob_store, file_name)])\n",
        "titanic_file_dataset = Dataset.File.from_files(path=[(def_blob_store, file_name)])\n",
        "\n",
        "step1_input = titanic_file_dataset.as_named_input(\"file_input\").as_hdfs()\n",
        "step1_output = HDFSOutputDatasetConfig(destination=(def_blob_store,\"prepared-dataset\")).register_on_complete(name=\"titanic-pipeline-prepared\")\n",
        "step2_input = step1_output.as_input(\"step2_input\").as_download()\n",
        "\n",
        "from azureml.core.environment import Environment\n",
        "env = Environment(name=\"myenv\")\n",
        "env.python.conda_dependencies.add_pip_package(\"azureml-core==1.20.0\")\n",
        "\n",
        "\n",
        "step_1 = SynapseSparkStep(name = 'synapse-spark',\n",
        "                          file = 'dataprep.py',\n",
        "                          source_directory=\"./code\", \n",
        "                          inputs=[step1_input],\n",
        "                          outputs=[step1_output],\n",
        "                          arguments = [\"--file_input\", step1_input,\n",
        "                                       \"--output_dir\", step1_output],\n",
        "                          compute_target = synapse_compute_name,\n",
        "                          driver_memory = \"32g\",\n",
        "                          driver_cores = 2,\n",
        "                          executor_memory = \"32g\",\n",
        "                          executor_cores = 1,\n",
        "                          num_executors = 2,\n",
        "                          environment = env,\n",
        "                          allow_reuse=True)\n",
        "\n",
        "step_2 = PythonScriptStep(script_name=\"train.py\",\n",
        "                          arguments=[step2_input],\n",
        "                          inputs=[step2_input],\n",
        "                          compute_target=cpu_cluster_name,\n",
        "                          source_directory=\"./code\",\n",
        "                          allow_reuse=True);\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[step_1, step_2])\n",
        "pipeline_run = pipeline.submit('synapse-pipeline', regenerate_outputs=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "yunzhan"
      }
    ],
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
